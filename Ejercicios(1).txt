1==== Búsqueda de Línea ====
Ejercicio 1:
→ Datos para realizarlo:
- Función: f(α) = (α - 1)^2 + 3
- Intervalo: [0, 2]
- Tolerancia = 1e-6, Iteraciones Máximas = 60
→ Valor esperado: α ≈ 1.0

Ejercicio 2:
→ Datos para realizarlo:
- Función: f(α) = (α - 0.5)^2 + sin(3*α)
- Intervalo: [0, 1]
- Tolerancia = 1e-6, Iteraciones Máximas = 70
→ Valor esperado: α ≈ 0.5

Ejercicio 3:
→ Datos para realizarlo:
- Función: f(α) = log(1 + α^2)
- Intervalo: [0, 3]
- Tolerancia = 1e-6, Iteraciones Máximas = 80
→ Valor esperado: α ≈ 0.0


2==== Descenso de Gradiente ====
Método: Gradiente Básico
Ejercicio 1:
→ Datos para realizarlo:
- Función objetivo: (x1 - 1)^2 + (x2 + 2)^2
- x0 = [2.5, -4], learning_rate = 0.02, max_iter = 1000
→ Valor esperado: x1 = 1, x2 = -2

Ejercicio 2:
→ Datos para realizarlo:
- Función objetivo: x1^2 + x2^2 + x1*x2
- x0 = [1, 1], learning_rate = 0.03, max_iter = 1000
→ Valor esperado: x ≈ [0, 0]

Ejercicio 3:
→ Datos para realizarlo:
- Función objetivo: (x1 - 3)^2 + (x2 - 1)^2 + 5
- x0 = [0, 0], learning_rate = 0.04, max_iter = 1000
→ Valor esperado: x1 = 3, x2 = 1


3==== Método de Newton ====
Método: Newton Clásico
Ejercicio 1:
→ Datos para realizarlo:
- Función objetivo: x1^4 + x2^2 - 4*x1^2 - 4*x2
- Gradiente y Hessiana calculables
- x0 = [1, 1], tol = 1e-6, max_iter = 100
→ Valor esperado: mínimo local ≈ [0, 2]

Ejercicio 2:
→ Datos para realizarlo:
- Función objetivo: x1^2 - 2*x1*x2 + 4*x2^2
- Gradiente y Hessiana calculables
- x0 = [2, -1], tol = 1e-6, max_iter = 100
→ Valor esperado: mínimo global ≈ [0, 0]

Ejercicio 3:
→ Datos para realizarlo:
- Función objetivo: (x1-2)^2 + (x2-2)^2 + x1*x2
- Gradiente y Hessiana calculables
- x0 = [0, 0], tol = 1e-6, max_iter = 100
→ Valor esperado: mínimo ≈ [1, 1]


4==== Métodos Primales ====
Método: Gradiente Proyectado
Ejercicio 1:
→ Datos para realizarlo:
- Función objetivo: x1^2 + x2^2
- Restricción: x1 + x2 = 1
- Gradiente y jacobiano calculables
- x0 = [1, 1], learning_rate = 0.05
→ Valor esperado: [0.5, 0.5]

Ejercicio 2:
→ Datos para realizarlo:
- Función objetivo: (x1-1)^2 + (x2-2)^2
- Restricción: x1 - x2 = 0
- Gradiente y jacobiano calculables
- x0 = [1, 1], learning_rate = 0.05
→ Valor esperado: [1.5, 1.5]

Ejercicio 3:
→ Datos para realizarlo:
- Función objetivo: x1^2 + x2^2 + x1*x2
- Restricción: x1 + 2x2 = 2
- Gradiente y jacobiano calculables
- x0 = [1, 1], learning_rate = 0.05
→ Valor esperado: [0.4, 0.8]



5==== Penalización y Barrera ====
Método: Penalización Exterior
Ejercicio 1:
→ Datos para realizarlo:
- Función objetivo: x1^2 + x2^2
- Restricciones: x1 + x2 - 1 ≤ 0
- Penalización inicial = 1.0, tol = 1e-6
- x0 = [1, 1]
→ Valor esperado: [0.5, 0.5]

Ejercicio 2:
→ Datos para realizarlo:
- Función objetivo: (x1 - 2)^2 + (x2 - 3)^2
- Restricciones: x1 + x2 ≤ 5
- Penalización inicial = 1.0, tol = 1e-6
- x0 = [1, 1]
→ Valor esperado: [2, 3]

Ejercicio 3:
→ Datos para realizarlo:
- Función objetivo: x1^2 + x2^2
- Restricciones: x1 ≥ 0, x2 ≥ 0
- Penalización inicial = 1.0, tol = 1e-6
- x0 = [1, 1]
→ Valor esperado: [0, 0]



6==== Multiplicadores de Lagrange ====
Método: Newton-Lagrange
Ejercicio 1:
→ Datos para realizarlo:
- Función objetivo: (x1 - 2)^2 + (x2 - 1)^2
- Restricción: x1 + x2 = 3
- x0 = [1.0, 2.0], λ0 = 1.0, tol = 1e-6
→ Valor esperado: [2, 1]

Ejercicio 2:
→ Datos para realizarlo:
- Función objetivo: x1^2 + 2*x2^2
- Restricción: x1 - x2 = 1
- x0 = [0.0, 1.0], λ0 = 1.0, tol = 1e-6
→ Valor esperado: [1.5, 0.5]

Ejercicio 3:
→ Datos para realizarlo:
- Función objetivo: x1^2 + x2^2 + 3*x1
- Restricción: x1 + 2*x2 = 0
- x0 = [0.5, -0.5], λ0 = 1.0, tol = 1e-6
→ Valor esperado: ≈ [-1, 0.5]


