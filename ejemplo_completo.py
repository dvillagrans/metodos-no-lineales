"""
Ejemplos de uso de todos los mÃ©todos de optimizaciÃ³n implementados
"""

import numpy as np
import matplotlib.pyplot as plt
from metodos import (
    GradientDescentOptimizer,
    NewtonOptimizer,
    QuasiNewtonOptimizer,
    LineSearchOptimizer,
    ConjugateGradientOptimizer,
    ConstrainedOptimizer,
    PenaltyBarrierOptimizer,
    LagrangeMultiplierOptimizer
)


def ejemplo_funcion_objetivo(x, y):
    """FunciÃ³n de prueba: f(x,y) = (x-1)Â² + (y-2)Â²"""
    return (x - 1)**2 + (y - 2)**2


def ejemplo_gradiente(x, y):
    """Gradiente de la funciÃ³n de prueba"""
    return np.array([2*(x - 1), 2*(y - 2)])


def ejemplo_hessiana(x, y):
    """Hessiana de la funciÃ³n de prueba"""
    return np.array([[2, 0], [0, 2]])


def restriccion_igualdad(x, y):
    """RestricciÃ³n de igualdad: x + y - 2 = 0"""
    return x + y - 2


def restriccion_igualdad_jac(x, y):
    """Jacobiano de la restricciÃ³n de igualdad"""
    return np.array([1, 1])


def restriccion_desigualdad(x, y):
    """RestricciÃ³n de desigualdad: xÂ² + yÂ² - 4 â‰¤ 0"""
    return x**2 + y**2 - 4


def restriccion_desigualdad_jac(x, y):
    """Jacobiano de la restricciÃ³n de desigualdad"""
    return np.array([2*x, 2*y])


def main():
    print("ğŸ”§ DEMOSTRACIÃ“N DE MÃ‰TODOS DE OPTIMIZACIÃ“N NO LINEAL")
    print("=" * 60)
    
    # Punto inicial
    x0 = np.array([0.0, 0.0])
    
    print(f"ğŸ“ Punto inicial: {x0}")
    print(f"ğŸ“Š FunciÃ³n objetivo: f(x,y) = (x-1)Â² + (y-2)Â²")
    print(f"ğŸ¯ Ã“ptimo esperado: x* = [1, 2], f* = 0")
    print()
    
    # âœ… 1. BÃšSQUEDA DE LÃNEA (SECCIÃ“N ÃUREA)
    print("1ï¸âƒ£ BÃšSQUEDA DE LÃNEA - SECCIÃ“N ÃUREA")
    print("-" * 40)
    
    line_search = LineSearchOptimizer()
    result1 = line_search.golden_section(
        ejemplo_funcion_objetivo, ejemplo_gradiente, x0,
        max_iter=100, tol=1e-6
    )
    
    print(f"âœ… Resultado: x* = {result1['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result1['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones: {result1['iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result1['converged']}")
    print()
    
    # âœ… 2. DESCENSO DE GRADIENTE
    print("2ï¸âƒ£ DESCENSO DE GRADIENTE")
    print("-" * 40)
    
    grad_desc = GradientDescentOptimizer()
    result2 = grad_desc.gradient_descent(
        ejemplo_funcion_objetivo, ejemplo_gradiente, x0,
        learning_rate=0.1, max_iter=100, tol=1e-6
    )
    
    print(f"âœ… Resultado: x* = {result2['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result2['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones: {result2['iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result2['converged']}")
    print()
    
    # âœ… 3. MÃ‰TODO DE NEWTON
    print("3ï¸âƒ£ MÃ‰TODO DE NEWTON (CON HESSIANA)")
    print("-" * 40)
    
    newton = NewtonOptimizer()
    result3 = newton.newton_method(
        ejemplo_funcion_objetivo, ejemplo_gradiente, ejemplo_hessiana, x0,
        max_iter=100, tol=1e-6
    )
    
    print(f"âœ… Resultado: x* = {result3['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result3['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones: {result3['iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result3['converged']}")
    print()
    
    # âœ… 4. MÃ‰TODOS PRIMALES (GRADIENTE RESTRINGIDO)
    print("4ï¸âƒ£ MÃ‰TODOS PRIMALES - GRADIENTE PROYECTADO")
    print("-" * 40)
    
    constrained = ConstrainedOptimizer()
    result4 = constrained.projected_gradient(
        ejemplo_funcion_objetivo, ejemplo_gradiente,
        constraints_eq=[restriccion_igualdad],
        constraints_eq_jac=[restriccion_igualdad_jac],
        x0=np.array([1.0, 1.0]),  # Punto factible para x + y = 2
        learning_rate=0.1, max_iter=100, tol=1e-6
    )
    
    print(f"âœ… Resultado: x* = {result4['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result4['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones: {result4['iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result4['converged']}")
    print(f"âš–ï¸ ViolaciÃ³n restricciones: {result4['violations'][-1]:.6f}")
    print()
    
    # âœ… 5. MÃ‰TODOS DE PENALIZACIÃ“N
    print("5ï¸âƒ£ MÃ‰TODOS DE PENALIZACIÃ“N EXTERIOR")
    print("-" * 40)
    
    penalty = PenaltyBarrierOptimizer()
    result5 = penalty.exterior_penalty(
        ejemplo_funcion_objetivo, ejemplo_gradiente,
        constraints_eq=[restriccion_igualdad],
        x0=x0, penalty_start=1.0, max_outer_iter=10, max_inner_iter=50
    )
    
    print(f"âœ… Resultado: x* = {result5['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result5['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones externas: {result5['outer_iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result5['converged']}")
    print(f"âš–ï¸ ViolaciÃ³n restricciones: {result5['violations'][-1]:.6f}")
    print()
    
    # âœ… 6. MÃ‰TODO DE BARRERA LOGARÃTMICA
    print("6ï¸âƒ£ MÃ‰TODO DE BARRERA LOGARÃTMICA")
    print("-" * 40)
    
    try:
        # Punto factible para xÂ² + yÂ² â‰¤ 4
        x0_factible = np.array([0.5, 0.5])
        result6 = penalty.logarithmic_barrier(
            ejemplo_funcion_objetivo, ejemplo_gradiente,
            constraints_ineq=[restriccion_desigualdad],
            x0=x0_factible, barrier_start=1.0, max_outer_iter=10
        )
        
        print(f"âœ… Resultado: x* = {result6['x']:.4f}")
        print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result6['errors'][-1]:.6f}")
        print(f"ğŸ”„ Iteraciones externas: {result6['outer_iterations']}")
        print(f"âœ”ï¸ ConvergiÃ³: {result6['converged']}")
        print()
    except Exception as e:
        print(f"âš ï¸ Error en barrera logarÃ­tmica: {e}")
        print()
    
    # âœ… 7. MULTIPLICADORES DE LAGRANGE
    print("7ï¸âƒ£ MULTIPLICADORES DE LAGRANGE (NEWTON-KKT)")
    print("-" * 40)
    
    lagrange = LagrangeMultiplierOptimizer()
    result7 = lagrange.lagrange_newton(
        ejemplo_funcion_objetivo, ejemplo_gradiente, ejemplo_hessiana,
        constraints_eq=[restriccion_igualdad],
        constraints_eq_jac=[restriccion_igualdad_jac],
        x0=np.array([1.0, 1.0]), max_iter=50
    )
    
    print(f"âœ… Resultado: x* = {result7['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result7['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones: {result7['iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result7['converged']}")
    print(f"ğŸ›ï¸ Multiplicadores: Î» = {result7['lambda']:.4f}")
    print(f"âš–ï¸ ViolaciÃ³n KKT: {result7['violations'][-1]:.6f}")
    print()
    
    # âœ… 8. LAGRANGIANO AUMENTADO
    print("8ï¸âƒ£ LAGRANGIANO AUMENTADO")
    print("-" * 40)
    
    result8 = lagrange.augmented_lagrangian(
        ejemplo_funcion_objetivo, ejemplo_gradiente,
        constraints_eq=[restriccion_igualdad],
        constraints_eq_jac=[restriccion_igualdad_jac],
        x0=x0, max_outer_iter=10, max_inner_iter=50
    )
    
    print(f"âœ… Resultado: x* = {result8['x']:.4f}")
    print(f"ğŸ“ˆ Valor Ã³ptimo: f* = {result8['errors'][-1]:.6f}")
    print(f"ğŸ”„ Iteraciones externas: {result8['outer_iterations']}")
    print(f"âœ”ï¸ ConvergiÃ³: {result8['converged']}")
    print(f"ğŸ›ï¸ Multiplicadores: Î» = {result8['lambda_eq']:.4f}")
    print(f"âš–ï¸ ViolaciÃ³n restricciones: {result8['violations'][-1]:.6f}")
    print()
    
    print("ğŸ‰ Â¡TODOS LOS MÃ‰TODOS COMPLETADOS!")
    print("=" * 60)
    print("ğŸ“š RESUMEN DE MÃ‰TODOS IMPLEMENTADOS:")
    print("âœ… 1. BÃºsqueda de lÃ­nea (secciÃ³n Ã¡urea)")
    print("âœ… 2. Descenso de gradiente")
    print("âœ… 3. MÃ©todo de Newton (con hessiana)")
    print("âœ… 4. MÃ©todos primales (gradiente restringido)")
    print("âœ… 5. MÃ©todos de penalizaciÃ³n exterior")
    print("âœ… 6. MÃ©todos de barrera (logarÃ­tmica, inversa)")
    print("âœ… 7. Multiplicadores de Lagrange (Newton-KKT)")
    print("âœ… 8. Lagrangiano aumentado")
    print()
    print("ğŸ”¥ Â¡Tu colecciÃ³n de mÃ©todos de optimizaciÃ³n estÃ¡ COMPLETA!")


if __name__ == "__main__":
    main()
